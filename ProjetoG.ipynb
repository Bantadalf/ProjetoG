{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c126ee-1744-4a6c-b90f-b7cdb77b6bab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/ProjetoG/TB_CEP_BR_2018.csv</td><td>TB_CEP_BR_2018.csv</td><td>51395868</td><td>1709119089000</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/compras-1.csv</td><td>compras-1.csv</td><td>9309</td><td>1709119852000</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/compras-2.csv</td><td>compras-2.csv</td><td>9309</td><td>1709119980000</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/compras.csv</td><td>compras.csv</td><td>9309</td><td>1709119067000</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/compras_rejeitadas/</td><td>compras_rejeitadas/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/compras_rejeitadas.csv/</td><td>compras_rejeitadas.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/compras_tratadas/</td><td>compras_tratadas/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/compras_tratadas.parquet/</td><td>compras_tratadas.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/condicao_pagamento.csv</td><td>condicao_pagamento.csv</td><td>192</td><td>1709119067000</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/enderecos_fornecedores.parquet/</td><td>enderecos_fornecedores.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/fornecedores.parquet/</td><td>fornecedores.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/nfs_entrada.parquet/</td><td>nfs_entrada.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/pagamentos_efetuados-1.csv</td><td>pagamentos_efetuados-1.csv</td><td>1565</td><td>1709119723000</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/pagamentos_efetuados.csv</td><td>pagamentos_efetuados.csv</td><td>1565</td><td>1709119067000</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/prog_pagamentos.parquet/</td><td>prog_pagamentos.parquet/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/tipo_endereco-1.csv</td><td>tipo_endereco-1.csv</td><td>254</td><td>1709119659000</td></tr><tr><td>dbfs:/FileStore/tables/ProjetoG/tipo_endereco.csv</td><td>tipo_endereco.csv</td><td>254</td><td>1709119069000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/ProjetoG/TB_CEP_BR_2018.csv",
         "TB_CEP_BR_2018.csv",
         51395868,
         1709119089000
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/compras-1.csv",
         "compras-1.csv",
         9309,
         1709119852000
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/compras-2.csv",
         "compras-2.csv",
         9309,
         1709119980000
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/compras.csv",
         "compras.csv",
         9309,
         1709119067000
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/compras_rejeitadas/",
         "compras_rejeitadas/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/compras_rejeitadas.csv/",
         "compras_rejeitadas.csv/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/compras_tratadas/",
         "compras_tratadas/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/compras_tratadas.parquet/",
         "compras_tratadas.parquet/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/condicao_pagamento.csv",
         "condicao_pagamento.csv",
         192,
         1709119067000
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/enderecos_fornecedores.parquet/",
         "enderecos_fornecedores.parquet/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/fornecedores.parquet/",
         "fornecedores.parquet/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/nfs_entrada.parquet/",
         "nfs_entrada.parquet/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/pagamentos_efetuados-1.csv",
         "pagamentos_efetuados-1.csv",
         1565,
         1709119723000
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/pagamentos_efetuados.csv",
         "pagamentos_efetuados.csv",
         1565,
         1709119067000
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/prog_pagamentos.parquet/",
         "prog_pagamentos.parquet/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/tipo_endereco-1.csv",
         "tipo_endereco-1.csv",
         254,
         1709119659000
        ],
        [
         "dbfs:/FileStore/tables/ProjetoG/tipo_endereco.csv",
         "tipo_endereco.csv",
         254,
         1709119069000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/FileStore/tables/ProjetoG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc88dd4-ad00-4064-b812-e245c0e21a53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------+\n|ID_TIPO_ENDERECO|           DESCRICAO|      SIGLA|\n+----------------+--------------------+-----------+\n|               1|            COBRANCA|        COB|\n|               2|             ENTREGA|        ENT|\n|               3|         FATURAMENTO|        FAT|\n|               4|    COBRANCA/ENTREGA|    COB/ENT|\n|               5|COBRANCA/FATURAMENTO|    COB/FAT|\n|               6| ENTREGA/FATURAMENTO|    ENT/FAT|\n|               7|COBRANCA/ENTREGA/...|COB/ENT/FAT|\n|               8|          FORNECEDOR|       FORN|\n+----------------+--------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#%% Importing the libraries\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "#%% Initialize the SparkSession\n",
    "SPARK = SparkSession.builder \\\n",
    "    .appName(\"Load Tipo Endereco Table\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logging.basicConfig()\n",
    "LOGGER = logging.getLogger(\"pyspark\")\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "#%% Load the function to compute the stage data for the table 'tipo_pagamento'\n",
    "def compute_tipo_endereco(spark: SparkSession):\n",
    "    TIPO_ENDERECO_FILEPATH = f'/FileStore/tables/ProjetoG/tipo_endereco.csv'\n",
    "\n",
    "    df = spark.read.csv(TIPO_ENDERECO_FILEPATH, header=True, sep=',')\n",
    "    \n",
    "    # Data cleaning\n",
    "    df = df.withColumn('nome_tipo_endereco', F.upper(F.col('nome_tipo_endereco')))\n",
    "    df = df.withColumn('nome_tipo_endereco', F.trim(F.col('nome_tipo_endereco')))\n",
    "    \n",
    "    df = df.withColumn('sigla_endereco', F.upper(F.col('sigla_endereco')))\n",
    "    df = df.withColumn('sigla_endereco', F.trim(F.col('sigla_endereco')))\n",
    "    \n",
    "    \n",
    "    # Column renaming\n",
    "    df = df.withColumnRenamed('id_tipo_endereco', 'ID_TIPO_ENDERECO')\n",
    "    df = df.withColumnRenamed('nome_tipo_endereco', 'DESCRICAO')\n",
    "    df = df.withColumnRenamed('sigla_endereco', 'SIGLA')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_tp_endereco = compute_tipo_endereco(SPARK)\n",
    "df_tp_endereco.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8009128d-72ed-4892-ab75-922adfd12fd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------+\n|ID_CONDICAO|           DESCRICAO|QTD_PARCELAS|\n+-----------+--------------------+------------+\n|          1|             A VISTA|           1|\n|          2|             30 DIAS|           1|\n|          3|          30/60 DIAS|           2|\n|          4|       30/60/90 DIAS|           3|\n|          5|     ENTRADA/30 DIAS|           2|\n|          6|  ENTRADA/30/60 DIAS|           3|\n|          7|ENTRADA/30/60/90 ...|           4|\n+-----------+--------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#%% Importing the libraries\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "#%% Initialize the SparkSession\n",
    "SPARK = SparkSession.builder \\\n",
    "    .appName(\"Load CONDICAO_PAGAMENTO Table\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logging.basicConfig()\n",
    "LOGGER = logging.getLogger(\"pyspark\")\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "#%% Load compute functions \n",
    "def compute_condicao_pagamento(spark: SparkSession):\n",
    "    COND_PAG_FILEPATH = f'/FileStore/tables/ProjetoG/condicao_pagamento.csv'\n",
    "\n",
    "    df = spark.read.csv(COND_PAG_FILEPATH, header=True, sep=',')\n",
    "    df = df.select('ID_CONDICAO', 'DESCRICAO', 'QTD_PARCELAS')\n",
    "    df = df.withColumn('DESCRICAO', F.upper(F.trim(F.col('DESCRICAO'))))\n",
    "    \n",
    "    df = df.withColumn('QTD_PARCELAS', F.col('QTD_PARCELAS').cast(T.IntegerType()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_condpag = compute_condicao_pagamento(SPARK)\n",
    "df_condpag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d109b30-278e-4533-a85e-c43f86dc0a7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+--------------+------------+--------------------+\n|     CEP| UF|        CIDADE|      BAIRRO|          LOGRADOURO|\n+--------+---+--------------+------------+--------------------+\n|23085680| RJ|RIO DE JANEIRO|CAMPO GRANDE| RUA CHARLES DICKENS|\n|23085690| RJ|RIO DE JANEIRO|CAMPO GRANDE|       PRAÇA CENTRAL|\n|23085700| RJ|RIO DE JANEIRO|CAMPO GRANDE|RUA BOM JESUS DA ...|\n|23085710| RJ|RIO DE JANEIRO|CAMPO GRANDE|   RUA PRIMEIRA CRUZ|\n|23085720| RJ|RIO DE JANEIRO|CAMPO GRANDE|         RUA DONEGAL|\n|23085730| RJ|RIO DE JANEIRO|CAMPO GRANDE|    RUA POUSO ALEGRE|\n|23085740| RJ|RIO DE JANEIRO|CAMPO GRANDE|           RUA EMAUS|\n|23085750| RJ|RIO DE JANEIRO|CAMPO GRANDE|RUA OSVALDO SENTO SÉ|\n|23085760| RJ|RIO DE JANEIRO|CAMPO GRANDE|RUA CORONEL JOSÉ ...|\n|23085770| RJ|RIO DE JANEIRO|CAMPO GRANDE|RUA JERÔNIMO BARB...|\n+--------+---+--------------+------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#%% Importing the libraries\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "#%% Initialize the SparkSession\n",
    "SPARK = SparkSession.builder \\\n",
    "    .appName(\"Load CEP Table\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logging.basicConfig()\n",
    "LOGGER = logging.getLogger(\"pyspark\")\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "#%% Load the function to compute the stage data for the file 'cep'\n",
    "def compute_cep_table(spark: SparkSession):\n",
    "    CEP_FILEPATH = f'/FileStore/tables/ProjetoG/TB_CEP_BR_2018.csv'\n",
    "    SCHEMA = T.StructType([\n",
    "        T.StructField('CEP', T.IntegerType(), True),\n",
    "        T.StructField('UF', T.StringType(), True),\n",
    "        T.StructField('CIDADE', T.StringType(), True),\n",
    "        T.StructField('BAIRRO', T.StringType(), True),\n",
    "        T.StructField('LOGRADOURO', T.StringType(), True),\n",
    "    ])\n",
    "\n",
    "    df = spark.read.csv(CEP_FILEPATH, header=False, sep=';', schema=SCHEMA)\n",
    "    \n",
    "    df = df.withColumn('UF', F.trim(F.upper(F.col('UF'))))\n",
    "    df = df.withColumn('CIDADE', F.trim(F.upper(F.col('CIDADE'))))\n",
    "    df = df.withColumn('BAIRRO', F.trim(F.upper(F.col('BAIRRO'))))\n",
    "    df = df.withColumn('LOGRADOURO', F.trim(F.upper(F.col('LOGRADOURO'))))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_cep = compute_cep_table(SPARK)\n",
    "df_cep.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9f42670-11ce-4618-af9e-90ec10e09253",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+-------------------+---------+------------+---------+-------------+-----------+------------+--------+------------------+--------+------------+-----------+-------------+------------------+\n|     NOME_FORNECEDOR|CNPJ_FORNECEDOR|    EMAIL_FORNECEDOR|TELEFONE_FORNECEDOR|NUMERO_NF|DATA_EMISSAO|VALOR_NET|VALOR_TRIBUTO|VALOR_TOTAL|   NOME_ITEM|QTD_ITEM|CONDICAO_PAGAMENTO|     CEP|NUM_ENDERECO|COMPLEMENTO|TIPO_ENDERECO|DATA_PROCESSAMENTO|\n+--------------------+---------------+--------------------+-------------------+---------+------------+---------+-------------+-----------+------------+--------+------------------+--------+------------+-----------+-------------+------------------+\n|        3R PETROLEUM| 12091809000155|dbreakwell2d@unes...|        286-44-7158|  9569521|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|   ENTRADA/30 DIAS|12605180|         170|    LOTE 74|   fornecedor|        2023-11-13|\n|         ENAUTA PART| 11669021000110|dbreakwell2d@unes...|        286-44-7158|  9375275|  2023-07-01|    70000|         3000|      73000|       Azera|       1|     30/60/90 DIAS|12605180|         170|    LOTE 74|   fornecedor|        2023-11-13|\n|BOSCO PARKER AND ...| 12528708000107|wglendinningk@eur...|        835-83-3066|  6402186|  2023-07-01|   110000|         5000|     115000|      Cooper|       1|           30 DIAS|12605170|         798|    LOTE 72|   fornecedor|        2023-11-13|\n|BOGISICH O'KEEFE ...| 82643537000134|  bogissfh.ss@ss.com|        742-63-3038|  8934386|  2023-07-01|   150000|         7000|     157000|      Tacoma|       1|   ENTRADA/30 DIAS|12605160|         693|    LOTE 80|   fornecedor|        2023-11-13|\n|           EXCELSIOR| 95426862000197|oschroeder2k@ocn....|        133-95-3943|  1123305|  2023-07-01|    70000|         3000|      73000|       Azera|       1|ENTRADA/30/60 DIAS|12605140|         445|    LOTE 57|   fornecedor|        2023-11-13|\n|        DMFINANCEIRA| 91669747000192|twillman2g@howstu...|        612-34-0166|  1721112|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|        30/60 DIAS|41301050|         349|    LOTE 68|   fornecedor|        2023-11-13|\n|        CYRELA REALT| 73178600000118|twillman2g@howstu...|        612-34-0166|  3299503|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|        30/60 DIAS|41301050|         349|    LOTE 68|   fornecedor|        2023-11-13|\n|                EVEN| 43470988000165|  ahancock25@wsj.com|        214-28-9058|  8041961|  2023-07-01|   110000|         5000|     115000|      Cooper|       1|           A VISTA|12605130|          99|        N/A|   fornecedor|        2023-11-13|\n|         ESPACOLASER| 26659061000159| lallans1x@state.gov|        778-53-4836|  3865995|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|           A VISTA|41300760|         655|        N/A|   fornecedor|        2023-11-13|\n|          DIRECIONAL| 16614075000100|twillman2g@howstu...|        612-34-0166|  7292216|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|        30/60 DIAS|41301050|         349|    LOTE 68|   fornecedor|        2023-11-13|\n|             ESTAPAR| 60537263000166|bhallad2@parallel...|        682-90-7397|  4598826|  2023-07-01|   150000|         7000|     157000|      Tacoma|       1|           A VISTA|41300770|          45|    LOTE 59|   fornecedor|        2023-11-13|\n|              DOHLER| 84683408000103|wglendinningk@eur...|        835-83-3066|  6725816|  2023-07-01|   110000|         5000|     115000|      Cooper|       1|           30 DIAS|12605170|         798|    LOTE 72|   fornecedor|        2023-11-13|\n|             ESTRELA| 61082004000150| mmundow1s@amazon.de|        533-64-8495|  8005113|  2023-07-01|   300000|        15000|     315000|     Boxster|       1|     30/60/90 DIAS|41300780|         221|    LOTE 02|   fornecedor|        2023-11-13|\n|             ETERNIT| 61092037000181|myelden1u@usatoda...|        763-57-5758|  4088238|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|     30/60/90 DIAS|12605120|         917|    LOTE 70|   fornecedor|        2023-11-13|\n|               DIMED| 92665611000177|dbreakwell2d@unes...|        286-44-7158|  8736964|  2023-07-01|    70000|         3000|      73000|       Azera|       1|     30/60/90 DIAS|12605180|         170|    LOTE 74|   fornecedor|        2023-11-13|\n|             EUCATEX| 56643018000166|ccaudrey27@hostga...|        722-35-9907|  6096104|  2023-07-01|   110000|         5000|     115000|      Cooper|       1|   ENTRADA/30 DIAS|12605070|        1223|        N/A|   fornecedor|        2023-11-13|\n|          EMBPAR S/A| 42331462000131|  bogissfh.ss@ss.com|        742-63-3038|  1639321|  2023-07-01|   150000|         7000|     157000|      Tacoma|       1|   ENTRADA/30 DIAS|12605160|         693|    LOTE 80|   fornecedor|        2023-11-13|\n|        EUROFARMA SA| 61190096000192|hhaddington17@cbc.ca|        201-38-1891|  6718140|  2023-07-01|   150000|         7000|     157000|      Tacoma|       1|     30/60/90 DIAS|41301055|        1248|        N/A|   fornecedor|        2023-11-13|\n|              ENJOEI| 16922038000151|myelden1u@usatoda...|        763-57-5758|  5358109|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|     30/60/90 DIAS|12605120|         917|    LOTE 70|   fornecedor|        2023-11-13|\n|          CVC BRASIL| 10760260000119| carrol.1234@fds.com|        632-63-6534|  9779142|  2023-07-01|   150000|         7000|     157000|      Tacoma|       1|           A VISTA|12605200|         997|    LOTE 92|   fornecedor|        2023-11-13|\n+--------------------+---------------+--------------------+-------------------+---------+------------+---------+-------------+-----------+------------+--------+------------------+--------+------------+-----------+-------------+------------------+\nonly showing top 20 rows\n\n+---------------+---------------+--------------------+-------------------+---------+------------+---------+-------------+-----------+------------+--------+------------------+--------+------------+-----------+-------------+------------------+--------------------+\n|NOME_FORNECEDOR|CNPJ_FORNECEDOR|    EMAIL_FORNECEDOR|TELEFONE_FORNECEDOR|NUMERO_NF|DATA_EMISSAO|VALOR_NET|VALOR_TRIBUTO|VALOR_TOTAL|   NOME_ITEM|QTD_ITEM|CONDICAO_PAGAMENTO|     CEP|NUM_ENDERECO|COMPLEMENTO|TIPO_ENDERECO|DATA_PROCESSAMENTO|     inconsistencies|\n+---------------+---------------+--------------------+-------------------+---------+------------+---------+-------------+-----------+------------+--------+------------------+--------+------------+-----------+-------------+------------------+--------------------+\n|    ENERGIAS BR|  3983431000103| lallans1x@state.gov|        778-53-4836|  4576383|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|           A VISTA|41300760|         655|        N/A|   fornecedor|        2023-11-13|        invalid CNPJ|\n|          ENEVA|  4423567000121|myelden1u@usatoda...|        763-57-5758|  6299298|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|     30/60/90 DIAS|12605120|         917|    LOTE 70|   fornecedor|        2023-11-13|        invalid CNPJ|\n|      EQTL PARA|  4895728000180|dbreakwell2d@unes...|        286-44-7158|  7311128|  2023-07-01|    70000|         3000|      73000|       Azera|       1|     30/60/90 DIAS|12605180|         170|    LOTE 74|   fornecedor|        2023-11-13|        invalid CNPJ|\n|      ELETROPAR|  1104937000170|oschroeder2k@ocn....|        133-95-3943|  6585245|  2023-07-01|    70000|         3000|      73000|       Azera|       1|ENTRADA/30/60 DIAS|12605140|         445|    LOTE 57|   fornecedor|        2023-11-13|        invalid CNPJ|\n|     ELETROBRAS|     1180000126|ccaudrey27@hostga...|        722-35-9907|  4862846|  2023-07-01|   110000|         5000|     115000|      Cooper|       1|   ENTRADA/30 DIAS|12605070|        1223|        N/A|   fornecedor|        2023-11-13|        invalid CNPJ|\n|     EQUATORIAL|  3220438000173|rcockman14@twitpi...|        313-34-5254|  4884454|  2023-07-01|   300000|        15000|     315000|     Boxster|       1|     30/60/90 DIAS|12605210|         453|    LOTE 41|   fornecedor|        2023-11-13|        invalid CNPJ|\n|   DESKTOPSIGMA|  8170849000115|ablazewicz2q@drup...|        329-18-0427|  7079969|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|ENTRADA/30/60 DIAS|12605150|         133|        N/A|   fornecedor|        2023-11-13|        invalid CNPJ|\n|     DEXXOS PAR|  2193750000152|wglendinningk@eur...|               NULL|     NULL|  2023-07-01|   110000|         5000|     115000|      Cooper|       1|           30 DIAS|12605170|         798|    LOTE 72|   fornecedor|        2023-11-13|null column(s), i...|\n|       CURY S/A|  8797760000183|myelden1u@usatoda...|        763-57-5758|  3301351|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|     30/60/90 DIAS|12605120|         917|    LOTE 70|   fornecedor|        2023-11-13|        invalid CNPJ|\n|        DOTZ SA| 18174270000184|dbreakwell2d@unes...|        286-44-7158|  4047651|  2023-07-01|    70000|         3000|      73000|        NULL|       1|              NULL|12605180|         170|    LOTE 74|   fornecedor|        2023-11-13|      null column(s)|\n|       CTC S.A.|   698138100011|azanussii1m@vimeo...|        691-01-7572|  5804444|  2023-07-01|    70000|         3000|      73000|       Azera|       1|           A VISTA|12605110|          11|    LOTE 91|   fornecedor|        2023-11-13|        invalid CNPJ|\n|       ENERGISA|   864214000106|bhallad2@parallel...|        682-90-7397|  8038785|  2023-07-01|   150000|         7000|     157000|      Tacoma|       1|           A VISTA|41300770|          45|    LOTE 59|   fornecedor|        2023-11-13|        invalid CNPJ|\n|    ELETROMIDIA|  9347516000181|  ahancock25@wsj.com|        214-28-9058|  7347699|  2023-07-01|   110000|         5000|     115000|      Cooper|       1|           A VISTA|12605130|          99|        N/A|   fornecedor|        2023-11-13|        invalid CNPJ|\n|          EZTEC|  8312229000173|ablazewicz2q@drup...|        329-18-0427|  7709747|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|ENTRADA/30/60 DIAS|12605150|         133|        N/A|   fornecedor|        2023-11-13|        invalid CNPJ|\n|   ENGIE BRASIL|  2474103000119|ccaudrey27@hostga...|        722-35-9907|  6822613|  2023-07-01|   110000|         5000|     115000|      Cooper|       1|   ENTRADA/30 DIAS|12605070|        1223|        N/A|   fornecedor|        2023-11-13|        invalid CNPJ|\n|   3R PETROLEUM| 12091809000155|dbreakwell2d@unes...|        286-44-7158|  9569520|  2023-07-01|    70000|         3000|      73000|       Azera|       1|     30/60/90 DIAS|12605180|         170|    LOTE 74|   fornecedor|        2023-11-13|          duplicated|\n|           NULL| 61486650000183|oschroeder2k@ocn....|        133-95-3943|  6125610|  2023-07-01|    70000|         3000|      73000|       Azera|       1|ENTRADA/30/60 DIAS|12605140|         445|    LOTE 57|   fornecedor|        2023-11-13|      null column(s)|\n|         FLEURY| 60840055000131|dbreakwell2d@unes...|        286-44-7158|  8571525|  2023-07-01|   110000|         5000|     115000|      Cooper|       1|   ENTRADA/30 DIAS|12605180|         170|    LOTE 74|   fornecedor|        2023-11-13|          duplicated|\n|   DTCOM DIRECT|  3303999000136|myelden1u@usatoda...|        763-57-5758|  6302145|  2023-07-01|   180000|         9000|       NULL|Escalade ESV|       1|     30/60/90 DIAS|12605120|         917|    LOTE 70|   fornecedor|        2023-11-13|null column(s), i...|\n|           EMAE|  2302101000142|ablazewicz2q@drup...|        329-18-0427|  7185140|  2023-07-01|   180000|         9000|     189000|Escalade ESV|       1|ENTRADA/30/60 DIAS|12605150|         133|        N/A|   fornecedor|        2023-11-13|        invalid CNPJ|\n+---------------+---------------+--------------------+-------------------+---------+------------+---------+-------------+-----------+------------+--------+------------------+--------+------------+-----------+-------------+------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#%% Importing the libraries\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import re\n",
    "\n",
    "#%% Initialize the SparkSession\n",
    "SPARK = SparkSession.builder \\\n",
    "    .appName(\"Load Staging Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logging.basicConfig()\n",
    "LOGGER = logging.getLogger(\"pyspark\")\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "def cnpj_valido(cnpj):\n",
    "    cnpj = re.sub(r'[^0-9]', '', cnpj)\n",
    "    if len(cnpj) !=14:\n",
    "        return False\n",
    "    \n",
    "    total = 0 \n",
    "    resto = 0 \n",
    "    digito_verificador_1 = 0\n",
    "    digito_verificador_2 = 0\n",
    "    multiplicadores1 = [5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "    multiplicadores2 = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "\n",
    "    for i in range(0,12,1):\n",
    "        total += int(cnpj[i]) * int(multiplicadores1[i])\n",
    "    resto = total % 11\n",
    "    \n",
    "    if resto < 2:\n",
    "        digito_verificador_1 = 0\n",
    "    else:\n",
    "        digito_verificador_1 = 11 - resto\n",
    "\n",
    "    total = 0\n",
    "    resto = 0\n",
    "\n",
    "    for i in range(0,13,1):\n",
    "        total += int(cnpj[i]) * int(multiplicadores2[i])\n",
    "\n",
    "    resto = total % 11\n",
    "    if resto < 2:\n",
    "        digito_verificador_2 = 0 \n",
    "    else:\n",
    "        digito_verificador_2 = 11 - resto\n",
    "\n",
    "    return cnpj[-2:] == str(digito_verificador_1) + str(digito_verificador_2) \n",
    "    cnpj_valido = udf(cnpj_valido, T.BooleanType())\n",
    "\n",
    "def compute_compras_stg(spark: SparkSession, df_ceps: DataFrame):\n",
    "    \"\"\"Compute the stage data for the table 'compras' and return a DataFrame with the stage data and a rejected DataFrame with the rejected data and the inconsistency type.\"\"\"\n",
    "    COMPRAS_FILEPATH = f'/FileStore/tables/ProjetoG/compras.csv'\n",
    "    \n",
    "    def subtract_by_index(df: DataFrame, df_to_subtract: DataFrame) -> DataFrame:\n",
    "        \"\"\"Subtract the rows of a DataFrame by the index column.\"\"\"\n",
    "        return df.join(df_to_subtract, df['index'] == df_to_subtract['index'], 'left_anti').select(df.columns)\n",
    "    \n",
    "    DF = spark.read.csv(COMPRAS_FILEPATH, header=True)\n",
    "    df = DF\n",
    "        \n",
    "    # Trim all columns \n",
    "    for column in df.columns:\n",
    "        df = df.withColumn(column, F.trim(F.col(column)))\n",
    "        \n",
    "    # Add a temp index column\n",
    "    df = df.withColumn('index', F.monotonically_increasing_id())\n",
    "    \n",
    "    # NOME_FORNECEDOR column treatment\n",
    "    df = df.withColumn('NOME_FORNECEDOR', F.upper(F.col('NOME_FORNECEDOR')))\n",
    "    df = df.withColumn('NOME_FORNECEDOR', F.trim(F.col('NOME_FORNECEDOR')))\n",
    "    \n",
    "    # CONDICAO_PAGAMENTO column treatment\n",
    "    df = df.withColumn('CONDICAO_PAGAMENTO', F.upper(F.col('CONDICAO_PAGAMENTO')))\n",
    "    df = df.withColumn('CONDICAO_PAGAMENTO', F.trim(F.col('CONDICAO_PAGAMENTO')))\n",
    "    \n",
    "    df = df.withColumn('CONDICAO_PAGAMENTO',\n",
    "        F.when(\n",
    "                    F.col('CONDICAO_PAGAMENTO') == 'ENTRADA/30/60/90 DIAS', '30/60/90 DIAS')\n",
    "            .when(  F.col('CONDICAO_PAGAMENTO') == 'A VISTA',               'A VISTA')\n",
    "            .when(  F.col('CONDICAO_PAGAMENTO') == '30 DIAS',               '30 DIAS')\n",
    "            .when(  F.col('CONDICAO_PAGAMENTO') == '30/60 DIAS',            '30/60 DIAS')\n",
    "            .when(  F.col('CONDICAO_PAGAMENTO') == 'ENTRADA/30 DIAS',       'ENTRADA/30 DIAS')\n",
    "            .when(  F.col('CONDICAO_PAGAMENTO') == 'ENTRADA/30/60 DIAS',    'ENTRADA/30/60 DIAS')\n",
    "        .otherwise( F.col('CONDICAO_PAGAMENTO') + ' (invalid)')\n",
    "    )\n",
    "    \n",
    "    # COMPLEMENTO column treatment\n",
    "    df = df.na.fill('N/A', subset=['COMPLEMENTO'])\n",
    "\n",
    "    # Compute a DataFrame with the nulls \n",
    "    df_not_null = df.dropna(how='any', subset=[column for column in df.columns if column != 'COMPLEMENTO'])\n",
    "    \n",
    "    df_nulls = (\n",
    "        subtract_by_index(df, df_not_null)\n",
    "        .withColumn('inconsistency', F.lit('null column(s)'))\n",
    "    )\n",
    "    \n",
    "    # Compute a DataFrame with the invalid payment condition\n",
    "    df_invalid_payment_condition = (\n",
    "        df\n",
    "        .filter(F.col('CONDICAO_PAGAMENTO').contains('(invalid)'))\n",
    "        .withColumn('inconsistency', F.lit('invalid payment condition'))\n",
    "    )\n",
    "    \n",
    "    # Compute a DataFrame with the invalid CEP\n",
    "    df_ceps = df_ceps.select('CEP')\n",
    "    df_ceps = df_ceps.withColumnRenamed('CEP', 'CEP_VALIDO')\n",
    "    \n",
    "    df_invalid_ceps = (\n",
    "        # Lookup the valid CEPs and filter the valid ones\n",
    "        df \n",
    "        .join(df_ceps, \n",
    "              df['CEP'].cast('int') == df_ceps['CEP_VALIDO'].cast('int'), \n",
    "              'left'\n",
    "        )\n",
    "        .filter(F.col('CEP_VALIDO').isNull())\n",
    "        .select(df.columns)\n",
    "        .withColumn('inconsistency', F.lit('invalid CEP'))\n",
    "    )\n",
    "    \n",
    "    # Compute a DataFrame with the invalid CNPJ\n",
    "    @F.udf(T.BooleanType())\n",
    "    def udf_is_cnpj_valid(cnpj: str) -> bool:\n",
    "        if cnpj is None:\n",
    "            return False\n",
    "        return cnpj_valido(cnpj)\n",
    "    spark.udf.register(\"udf_is_cnpj_valid\", udf_is_cnpj_valid)\n",
    "    \n",
    "    df = df.cache()\n",
    "    df_invalids_cnpj = df.filter(~udf_is_cnpj_valid(F.col('CNPJ_FORNECEDOR')))\n",
    "    df_invalids_cnpj = df_invalids_cnpj.withColumn('inconsistency', F.lit('invalid CNPJ'))\n",
    "    \n",
    "    # Remove the duplicates AND create a Dataframe with duplicated rows\n",
    "    df_duplicated = (\n",
    "        df\n",
    "        .groupBy(DF.columns)\n",
    "        .count()\n",
    "        .where(F.col('count') > 1)\n",
    "        .drop('count')\n",
    "        .withColumn('inconsistency', F.lit('duplicated'))\n",
    "    )\n",
    "    \n",
    "    # Remove all rejeted data by the index column\n",
    "    df = (\n",
    "        df\n",
    "        .transform(lambda df: subtract_by_index(df, df_nulls))\n",
    "        .transform(lambda df: subtract_by_index(df, df_invalid_payment_condition))\n",
    "        .transform(lambda df: subtract_by_index(df, df_invalids_cnpj))\n",
    "        .transform(lambda df: subtract_by_index(df, df_invalid_ceps))\n",
    "        .drop('index').dropDuplicates()\n",
    "    )\n",
    "    \n",
    "    df_rejected = (\n",
    "        df_duplicated.drop('index')\n",
    "        .union(df_nulls.drop('index'))\n",
    "        .union(df_invalid_payment_condition.drop('index'))\n",
    "        .union(df_invalids_cnpj.drop('index'))\n",
    "        .union(df_invalid_ceps.drop('index'))\n",
    "        .dropDuplicates()\n",
    "        .groupBy(df.columns)\n",
    "        .agg(F.concat_ws(\", \", F.collect_list(F.col('inconsistency'))).alias('inconsistencies'))\n",
    "    )\n",
    "\n",
    "    return df, df_rejected\n",
    "    \n",
    "df_compras, df_rejected = compute_compras_stg(SPARK, df_cep)\n",
    "df_compras.show()\n",
    "df_rejected.show()\n",
    "df_compras.write.parquet('/FileStore/tables/ProjetoG/compras_tratadas.parquet', mode = 'overwrite')\n",
    "df_rejected.write.csv('/FileStore/tables/ProjetoG/compras_rejeitadas.csv', header = 'True', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e84aea1-1df5-4f28-913e-c7b78da9cf98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------+--------------------+-----------+\n|ID_FORNECEDOR|                NOME|          CNPJ|               EMAIL|   TELEFONE|\n+-------------+--------------------+--------------+--------------------+-----------+\n|            1|           EXCELSIOR|95426862000197|oschroeder2k@ocn....|133-95-3943|\n|            2|BOSCO PARKER AND ...|12528708000107|wglendinningk@eur...|835-83-3066|\n|            3|                EVEN|43470988000165|  ahancock25@wsj.com|214-28-9058|\n|            4|        EUROFARMA SA|61190096000192|hhaddington17@cbc.ca|201-38-1891|\n|            5|              FLEURY|60840055000131|dbreakwell2d@unes...|286-44-7158|\n|            6|             ETERNIT|61092037000181|myelden1u@usatoda...|763-57-5758|\n|            7|BOGISICH O'KEEFE ...|82643537000134|  bogissfh.ss@ss.com|742-63-3038|\n|            8|              ENJOEI|16922038000151|myelden1u@usatoda...|763-57-5758|\n|            9|             FERBASA|15141799000103|wglendinningk@eur...|835-83-3066|\n|           10|        DMFINANCEIRA|91669747000192|twillman2g@howstu...|612-34-0166|\n|           11|        FER HERINGER|22266175000188|  bogissfh.ss@ss.com|742-63-3038|\n|           12|              DOHLER|84683408000103|wglendinningk@eur...|835-83-3066|\n|           13|        3R PETROLEUM|12091809000155|dbreakwell2d@unes...|286-44-7158|\n|           14|          EMBPAR S/A|42331462000131|  bogissfh.ss@ss.com|742-63-3038|\n|           15|        CYRELA REALT|73178600000118|twillman2g@howstu...|612-34-0166|\n|           16|         ENAUTA PART|11669021000110|dbreakwell2d@unes...|286-44-7158|\n|           17|             ESTRELA|61082004000150| mmundow1s@amazon.de|533-64-8495|\n|           18|         ESPACOLASER|26659061000159| lallans1x@state.gov|778-53-4836|\n|           19|               DIMED|92665611000177|dbreakwell2d@unes...|286-44-7158|\n|           20|          CVC BRASIL|10760260000119| carrol.1234@fds.com|632-63-6534|\n+-------------+--------------------+--------------+--------------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#%% Importing the libraries\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "#%% Initialize the SparkSession\n",
    "SPARK = SparkSession.builder \\\n",
    "    .appName(\"Load Fornecedores Table\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logging.basicConfig()\n",
    "LOGGER = logging.getLogger(\"pyspark\")\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "#%% Load the function to compute fornecedores dataframe\n",
    "def compute_fornecedores(spark: SparkSession):\n",
    "    COMPRAS_TRAT_FILEPATH = f'/FileStore/tables/ProjetoG/compras_tratadas.parquet'\n",
    "    \n",
    "    df = spark.read.parquet(COMPRAS_TRAT_FILEPATH)\n",
    "    \n",
    "    # Select the columns, drop duplicates and rename the columns\n",
    "    df = df.select('NOME_FORNECEDOR', 'CNPJ_FORNECEDOR', 'EMAIL_FORNECEDOR', 'TELEFONE_FORNECEDOR')\n",
    "    df = df.dropDuplicates()\n",
    "    \n",
    "    df = (\n",
    "        df\n",
    "        .withColumn('ID_FORNECEDOR', F.row_number().over(W.orderBy(F.monotonically_increasing_id())))\n",
    "        .withColumnRenamed('NOME_FORNECEDOR', 'NOME')\n",
    "        .withColumnRenamed('CNPJ_FORNECEDOR', 'CNPJ')\n",
    "        .withColumnRenamed('EMAIL_FORNECEDOR', 'EMAIL')\n",
    "        .withColumnRenamed('TELEFONE_FORNECEDOR', 'TELEFONE')\n",
    "        .select('ID_FORNECEDOR', 'NOME', 'CNPJ', 'EMAIL', 'TELEFONE')\n",
    "    )\n",
    "       \n",
    "    return df\n",
    "\n",
    "df_fornecedores = compute_fornecedores(SPARK)\n",
    "df_fornecedores.show()\n",
    "df_fornecedores.write.parquet('/FileStore/tables/ProjetoG/fornecedores.parquet', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a1667f-2bca-464b-b4b3-85515b6aeb96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------------+------------+-----------+\n|     CEP|ID_FORNECEDOR|ID_TIPO_ENDERECO|NUM_ENDERECO|COMPLEMENTO|\n+--------+-------------+----------------+------------+-----------+\n|41301050|           15|               8|         349|    LOTE 68|\n|12605180|            5|               8|         170|    LOTE 74|\n|41300780|           17|               8|         221|    LOTE 02|\n|41301050|           23|               8|         349|    LOTE 68|\n|12605120|            6|               8|         917|    LOTE 70|\n|12605200|           20|               8|         997|    LOTE 92|\n|12605130|            3|               8|          99|        N/A|\n|12605140|            1|               8|         445|    LOTE 57|\n|12605160|           14|               8|         693|    LOTE 80|\n|12605180|           16|               8|         170|    LOTE 74|\n|12605160|           11|               8|         693|    LOTE 80|\n|12605070|           21|               8|        1223|        N/A|\n|41300770|           22|               8|          45|    LOTE 59|\n|12605120|            8|               8|         917|    LOTE 70|\n|12605170|           12|               8|         798|    LOTE 72|\n|41301050|           10|               8|         349|    LOTE 68|\n|12605170|            2|               8|         798|    LOTE 72|\n|12605170|            9|               8|         798|    LOTE 72|\n|41301055|            4|               8|        1248|        N/A|\n|12605180|           13|               8|         170|    LOTE 74|\n+--------+-------------+----------------+------------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#%% Importing the libraries\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#%% Initialize the SparkSession\n",
    "SPARK = SparkSession.builder \\\n",
    "    .appName(\"Load Endereco Fornecedores Table\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logging.basicConfig()\n",
    "LOGGER = logging.getLogger(\"pyspark\")\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "#%% Load the function to compute dataframes\n",
    "def compute_compras_stg(spark: SparkSession):\n",
    "    \"\"\"Compute the dataframe for the compras_stg table.\"\"\"\n",
    "    COMPRAS_TRAT_FILEPATH = f'/FileStore/tables/ProjetoG/compras_tratadas.parquet'\n",
    "    return spark.read.parquet(COMPRAS_TRAT_FILEPATH)\n",
    "\n",
    "def compute_fornecedores_parquet(spark: SparkSession):\n",
    "    \"\"\"Compute the dataframe for the fornecedores table.\"\"\"\n",
    "    FORN_FILEPATH = f'/FileStore/tables/ProjetoG/fornecedores.parquet'\n",
    "    return spark.read.parquet(FORN_FILEPATH)\n",
    "\n",
    "def compute_enderecos_fornecedores_table(df_compras_stg: DataFrame, df_fornecedores: DataFrame, df_tipo_endereco: DataFrame):\n",
    "    \"\"\"Compute the dataframe for the enderecos_fornecedores table.\"\"\"\n",
    "    \n",
    "    df = df_compras_stg.select(\n",
    "            'CNPJ_FORNECEDOR',\n",
    "            'TIPO_ENDERECO',\n",
    "            'NUM_ENDERECO',\n",
    "            'COMPLEMENTO',\n",
    "            'CEP'\n",
    "        )\n",
    "    \n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    df = df.join(df_fornecedores, df['CNPJ_FORNECEDOR'] == df_fornecedores['CNPJ'], 'inner')\n",
    "    df = df.join(df_tipo_endereco, F.upper(df['TIPO_ENDERECO']) == df_tipo_endereco['DESCRICAO'], 'inner')\n",
    "    \n",
    "    df = df.select(\n",
    "        df_compras_stg['CEP'],\n",
    "        df_fornecedores['ID_FORNECEDOR'],\n",
    "        df_tipo_endereco['ID_TIPO_ENDERECO'],\n",
    "        df_compras_stg['NUM_ENDERECO'],   \n",
    "        df_compras_stg['COMPLEMENTO']\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_compras_trat = compute_compras_stg(SPARK)\n",
    "df_fornecedores_p = compute_fornecedores_parquet(SPARK)\n",
    "df_tipo_endereco =  df_tp_endereco\n",
    "\n",
    "df_endereco_fornecedores = compute_enderecos_fornecedores_table(df_compras_trat, df_fornecedores_p, df_tipo_endereco)\n",
    "df_endereco_fornecedores.show()\n",
    "df_endereco_fornecedores.write.parquet('/FileStore/tables/ProjetoG/enderecos_fornecedores.parquet', mode = 'overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10020bee-8267-49f2-984d-5a26aea0311b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------------+-----------+------------+---------+-------------+-----------+------------+--------+\n|ID_NF_ENTRADA|NUMERO_NF|ID_FORNECEDOR|ID_CONDICAO|DATA_EMISSAO|VALOR_NET|VALOR_TRIBUTO|VALOR_TOTAL|NOME_ITEM   |QTD_ITEM|\n+-------------+---------+-------------+-----------+------------+---------+-------------+-----------+------------+--------+\n|1            |9569521  |13           |5          |2023-07-01  |180000   |9000         |189000     |Escalade ESV|1       |\n|2            |9375275  |16           |4          |2023-07-01  |70000    |3000         |73000      |Azera       |1       |\n|3            |6402186  |2            |2          |2023-07-01  |110000   |5000         |115000     |Cooper      |1       |\n|4            |8934386  |7            |5          |2023-07-01  |150000   |7000         |157000     |Tacoma      |1       |\n|5            |1123305  |1            |6          |2023-07-01  |70000    |3000         |73000      |Azera       |1       |\n|6            |1721112  |10           |3          |2023-07-01  |180000   |9000         |189000     |Escalade ESV|1       |\n|7            |3299503  |15           |3          |2023-07-01  |180000   |9000         |189000     |Escalade ESV|1       |\n|8            |8041961  |3            |1          |2023-07-01  |110000   |5000         |115000     |Cooper      |1       |\n|9            |3865995  |18           |1          |2023-07-01  |180000   |9000         |189000     |Escalade ESV|1       |\n|10           |7292216  |23           |3          |2023-07-01  |180000   |9000         |189000     |Escalade ESV|1       |\n|11           |8571525  |5            |5          |2023-07-01  |110000   |5000         |115000     |Cooper      |1       |\n|12           |4598826  |22           |1          |2023-07-01  |150000   |7000         |157000     |Tacoma      |1       |\n|13           |6725816  |12           |2          |2023-07-01  |110000   |5000         |115000     |Cooper      |1       |\n|14           |8571524  |5            |4          |2023-07-01  |70000    |3000         |73000      |Azera       |1       |\n|15           |8005113  |17           |4          |2023-07-01  |300000   |15000        |315000     |Boxster     |1       |\n|16           |4088238  |6            |4          |2023-07-01  |180000   |9000         |189000     |Escalade ESV|1       |\n|17           |8736964  |19           |4          |2023-07-01  |70000    |3000         |73000      |Azera       |1       |\n|18           |7169715  |9            |2          |2023-07-01  |110000   |5000         |115000     |Cooper      |1       |\n|19           |6096104  |21           |5          |2023-07-01  |110000   |5000         |115000     |Cooper      |1       |\n|20           |1639321  |14           |5          |2023-07-01  |150000   |7000         |157000     |Tacoma      |1       |\n+-------------+---------+-------------+-----------+------------+---------+-------------+-----------+------------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#%% Importing the libraries\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#%% Initialize the SparkSession\n",
    "SPARK = SparkSession.builder \\\n",
    "    .appName(\"Load Notas Fiscais Table\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logging.basicConfig()\n",
    "LOGGER = logging.getLogger(\"pyspark\")\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "#%% Load the function to compute dataframes\n",
    "def compute_compras_stg(spark: SparkSession):\n",
    "    \"\"\"Compute the dataframe for the compras_stg table.\"\"\"\n",
    "    COMPRAS_TRAT_FILEPATH = f'/FileStore/tables/ProjetoG/compras_tratadas.parquet'\n",
    "    return spark.read.parquet(COMPRAS_TRAT_FILEPATH)\n",
    "\n",
    "def compute_fornecedores_parquet(spark: SparkSession):\n",
    "    \"\"\"Compute the dataframe for the fornecedores table.\"\"\"\n",
    "    FORN_FILEPATH = f'/FileStore/tables/ProjetoG/fornecedores.parquet'\n",
    "    return spark.read.parquet(FORN_FILEPATH)\n",
    "\n",
    "def compute_notas_fiscais_entrada_table(df_compras_stg: DataFrame, df_fornecedores: DataFrame, df_condicao_pagamento: DataFrame):\n",
    "    \"\"\"Compute the dataframe for the NOTAS_FISCAIS_ENTRADA table.\"\"\"\n",
    "    \n",
    "    df = df_compras_stg.select(\n",
    "        'NUMERO_NF',\n",
    "        'DATA_EMISSAO',\n",
    "        'VALOR_NET',\n",
    "        'VALOR_TRIBUTO',\n",
    "        'VALOR_TOTAL',\n",
    "        'NOME_ITEM',\n",
    "        'QTD_ITEM',\n",
    "        'CONDICAO_PAGAMENTO',\n",
    "        'CNPJ_FORNECEDOR'\n",
    "    )\n",
    "    df = df.withColumn('ID_NF_ENTRADA', F.row_number().over(W.orderBy(F.monotonically_increasing_id())))\n",
    "    df = df.join(df_fornecedores, df['CNPJ_FORNECEDOR'] == df_fornecedores['CNPJ'], 'inner')\n",
    "    df = df.join(df_condicao_pagamento, df['CONDICAO_PAGAMENTO'] == df_condicao_pagamento['DESCRICAO'], 'LEFT')    \n",
    "    \n",
    "    df = df.select(\n",
    "\n",
    "        df['ID_NF_ENTRADA'],\n",
    "        df['NUMERO_NF'],\n",
    "        df_fornecedores['ID_FORNECEDOR'],\n",
    "        df_condicao_pagamento['ID_CONDICAO'],\n",
    "        df['DATA_EMISSAO'],\n",
    "        df['VALOR_NET'],\n",
    "        df['VALOR_TRIBUTO'],\n",
    "        df['VALOR_TOTAL'],\n",
    "        df['NOME_ITEM'],\n",
    "        df['QTD_ITEM']\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn('DATA_EMISSAO', F.to_date(F.col('DATA_EMISSAO'), 'yyyy-MM-dd'))\n",
    "    df = df.withColumn('VALOR_NET', F.col('VALOR_NET').cast('decimal'))\n",
    "    df = df.withColumn('VALOR_TRIBUTO', F.col('VALOR_TRIBUTO').cast('decimal'))\n",
    "    df = df.withColumn('VALOR_TOTAL', F.col('VALOR_TOTAL').cast('decimal'))\n",
    "    df = df.withColumn('QTD_ITEM', F.col('QTD_ITEM').cast('integer'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "##You may execute this again.\n",
    "##df_compras_trat = compute_compras_stg(SPARK)\n",
    "##df_fornecedores = compute_fornecedores_table(SPARK)\n",
    "##df_condicao_pagamento = df_condpag\n",
    "\n",
    "df_nf_entrada = compute_notas_fiscais_entrada_table(df_compras_trat, df_fornecedores_p, df_condpag)\n",
    "df_nf_entrada.show(truncate=False)\n",
    "df_nf_entrada.write.parquet('/FileStore/tables/ProjetoG/nfs_entrada.parquet', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a452e7c-32a4-41a9-b2cb-ba703e510a1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------+-----------+------------------+----------------+\n|ID_PROG_PAG|ID_NF_ENTRADA|DATA_VENCIMENTO|NUM_PARCELA|VALOR_PARCELA     |STATUS_PAGAMENTO|\n+-----------+-------------+---------------+-----------+------------------+----------------+\n|1          |1            |2023-07-01     |1          |94500.00000000000 |PAGO            |\n|2          |1            |2023-08-01     |2          |94500.00000000000 |PAGO            |\n|3          |2            |2023-07-01     |1          |24333.33333333333 |PAGO            |\n|4          |2            |2023-08-01     |2          |24333.33333333333 |PAGO            |\n|5          |2            |2023-09-01     |3          |24333.33333333333 |PAGO            |\n|6          |3            |2023-07-01     |1          |115000.00000000000|PAGO            |\n|7          |4            |2023-07-01     |1          |78500.00000000000 |PAGO            |\n|8          |4            |2023-08-01     |2          |78500.00000000000 |PAGO            |\n|9          |5            |2023-07-01     |1          |24333.33333333333 |PAGO            |\n|10         |5            |2023-08-01     |2          |24333.33333333333 |PAGO            |\n|11         |5            |2023-09-01     |3          |24333.33333333333 |PAGO            |\n|12         |6            |2023-07-01     |1          |94500.00000000000 |PAGO            |\n|13         |6            |2023-08-01     |2          |94500.00000000000 |PAGO            |\n|14         |7            |2023-07-01     |1          |94500.00000000000 |PAGO            |\n|15         |7            |2023-08-01     |2          |94500.00000000000 |PAGO            |\n|16         |8            |2023-07-01     |1          |115000.00000000000|PAGO            |\n|17         |9            |2023-07-01     |1          |189000.00000000000|PAGO            |\n|18         |10           |2023-07-01     |1          |94500.00000000000 |PAGO            |\n|19         |10           |2023-08-01     |2          |94500.00000000000 |PAGO            |\n|20         |11           |2023-07-01     |1          |57500.00000000000 |PAGO            |\n+-----------+-------------+---------------+-----------+------------------+----------------+\nonly showing top 20 rows\n\n+-----------+-------------+---------------+-----------+------------------+\n|ID_HIST_PAG|ID_NF_ENTRADA|DATA_VENCIMENTO|NUM_PARCELA|VALOR_PARCELA     |\n+-----------+-------------+---------------+-----------+------------------+\n|1          |1            |2023-07-01     |1          |94500.00000000000 |\n|2          |1            |2023-08-01     |2          |94500.00000000000 |\n|3          |2            |2023-07-01     |1          |24333.33333333333 |\n|4          |2            |2023-08-01     |2          |24333.33333333333 |\n|5          |2            |2023-09-01     |3          |24333.33333333333 |\n|6          |3            |2023-07-01     |1          |115000.00000000000|\n|7          |4            |2023-07-01     |1          |78500.00000000000 |\n|8          |4            |2023-08-01     |2          |78500.00000000000 |\n|9          |5            |2023-07-01     |1          |24333.33333333333 |\n|10         |5            |2023-08-01     |2          |24333.33333333333 |\n|11         |5            |2023-09-01     |3          |24333.33333333333 |\n|12         |6            |2023-07-01     |1          |94500.00000000000 |\n|13         |6            |2023-08-01     |2          |94500.00000000000 |\n|14         |7            |2023-07-01     |1          |94500.00000000000 |\n|15         |7            |2023-08-01     |2          |94500.00000000000 |\n|16         |8            |2023-07-01     |1          |115000.00000000000|\n|17         |9            |2023-07-01     |1          |189000.00000000000|\n|18         |10           |2023-07-01     |1          |94500.00000000000 |\n|19         |10           |2023-08-01     |2          |94500.00000000000 |\n|20         |11           |2023-07-01     |1          |57500.00000000000 |\n+-----------+-------------+---------------+-----------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#%% Importing the libraries\n",
    "import logging\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "#%% Initialize the SparkSession\n",
    "SPARK = SparkSession.builder \\\n",
    "    .appName(\"Load Programacao Pagamento Table\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logging.basicConfig()\n",
    "LOGGER = logging.getLogger(\"pyspark\")\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "\n",
    "def compute_notas_fiscais_de_entrada(spark: SparkSession):\n",
    "    \"\"\"Compute the dataframe using a query\"\"\"\n",
    "    NFS_PATH = f'/FileStore/tables/ProjetoG/nfs_entrada.parquet'\n",
    "    \n",
    "    df = spark.read.parquet(NFS_PATH)\n",
    "    df = df.join(df_condpag, df['ID_CONDICAO'] == df_condpag['ID_CONDICAO'], 'INNER')\n",
    "    df = df.select(\n",
    "        df['ID_NF_ENTRADA'],\n",
    "        df['DATA_EMISSAO'],\n",
    "        df['VALOR_TOTAL'],\n",
    "        df_condpag['QTD_PARCELAS']\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def compute_programacao_pagamento_pendente(spark: SparkSession):\n",
    "    \"\"\"Compute the programcao_pagamento dataframe\"\"\"\n",
    "    \n",
    "    schema = df_notas_fiscais_entrada.schema\n",
    "\n",
    "    df = spark.createDataFrame([], schema)\n",
    "    \n",
    "    df = df.withColumn('STATUS_PAGAMENTO', F.lit('PENDENTE'))  # Default status is 'PENDENTE'\n",
    "    \n",
    "    return df\n",
    "\n",
    "#%% Load the function to compute the stage data for the table 'tipo_pagamento'\n",
    "def compute_programacao_pagamento(df_notas_fiscais_entrada: DataFrame, df_programacao_pagamento_pendente: DataFrame):\n",
    "    \"\"\"Compute the Programacao Pagamento dataframe using the Notas Fiscais de Entrada dataframe\"\"\"\n",
    "    \n",
    "    df = (\n",
    "        df_notas_fiscais_entrada\n",
    "        .withColumn(\"PARCELAS\", F.sequence(F.lit(1), F.col(\"QTD_PARCELAS\")))\n",
    "        .withColumn(\"NUM_PARCELA\", F.explode(F.col(\"PARCELAS\")))\n",
    "        .drop(\"PARCELAS\")\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\"VALOR_PARCELA\", F.col(\"VALOR_TOTAL\") / F.col(\"QTD_PARCELAS\"))\n",
    "    # I using the expr function because the F.add_months function not accept a column as integer parameter to increment the date \n",
    "    df = df.withColumn(\"DATA_VENCIMENTO\", F.expr(\"add_months(DATA_EMISSAO, NUM_PARCELA - 1)\"))\n",
    "    \n",
    "    df = df.select(\n",
    "        'ID_NF_ENTRADA',\n",
    "        'DATA_VENCIMENTO',\n",
    "        'NUM_PARCELA',\n",
    "        'VALOR_PARCELA'\n",
    "    )\n",
    "    \n",
    "    # All dates equal or minor than today are considered as \"PAGO\", else \"PENDENTE\"\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"STATUS_PAGAMENTO\",   \n",
    "                    F.when(F.col(\"DATA_VENCIMENTO\") <= F.current_date(), F.lit(\"PAGO\")) ##You can put '2023-07-01' to see how the column 'STATUS_PAGAMENTO' reacts\n",
    "                    .otherwise(F.lit(\"PENDENTE\")))\n",
    "    )\n",
    "\n",
    "    df = df.union(df_programacao_pagamento_pendente)\n",
    "    df = df.withColumn('ID_PROG_PAG', F.row_number().over(W.orderBy(F.monotonically_increasing_id())))\n",
    "    df = df.select(\n",
    "        'ID_PROG_PAG',\n",
    "        'ID_NF_ENTRADA',\n",
    "        'DATA_VENCIMENTO',\n",
    "        'NUM_PARCELA',\n",
    "        'VALOR_PARCELA',\n",
    "        'STATUS_PAGAMENTO'\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "df_notas_fiscais_entrada = compute_notas_fiscais_de_entrada(SPARK)\n",
    "df_programacao_pendente = compute_programacao_pagamento_pendente(SPARK)\n",
    "\n",
    "df_programacao_pagamentos = compute_programacao_pagamento(df_notas_fiscais_entrada, df_programacao_pendente)\n",
    "df_programacao_pagamentos.show(truncate=False)\n",
    "df_programacao_pagamentos.write.parquet('/FileStore/tables/ProjetoG/prog_pagamentos.parquet', mode = 'overwrite')\n",
    "\n",
    "df_historico_pagamentos = df_programacao_pagamentos.filter(F.col('STATUS_PAGAMENTO') == 'PAGO')\n",
    "df_historico_pagamentos = df_historico_pagamentos.withColumn('ID_HIST_PAG', F.row_number().over(W.orderBy(F.monotonically_increasing_id())))\n",
    "df_historico_pagamentos = df_historico_pagamentos.select(\n",
    "    'ID_HIST_PAG',\n",
    "    'ID_NF_ENTRADA',\n",
    "    'DATA_VENCIMENTO',\n",
    "    'NUM_PARCELA',\n",
    "    'VALOR_PARCELA'\n",
    ")\n",
    "df_historico_pagamentos.show(truncate=False)\n",
    "df_historico_pagamentos.write.parquet('/FileStore/tables/ProjetoG/hist_pagamentos.parquet', mode = 'overwrite')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ProjetoG",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
